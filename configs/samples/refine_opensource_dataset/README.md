# Refined open source dataset by Data-flow

We found that there are still some "bad" samples in existing processed datasets (e.g. RedPajama, The Pile.). So we use our Data-flow to refine them and try to feed them to LLMs for better performance.

We use simple 3-Ïƒ rule to set the hyperparameters for ops in each recipe.

## Before and after refining for Pretraining Text Dataset

| subset               |       #samples before       | #samples after | keep ratio | config link                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | source                  |
|----------------------|:---------------------------:|:--------------:|:----------:|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|
| arXiv                |          1,724,497          |   1,655,259    |   95.99%   | [redpajama-arxiv-refine.yaml](redpajama-arxiv-refine.yaml)                                                                                                                                                                                                                | Redpajama               |
| Books                |           205,182           |    195,983     |   95.51%   | [redpajama-book-refine.yaml](redpajama-book-refine.yaml)                                                                                                                                                                                                                   | Redpajama               |
| Wikipedia            |         29,834,171          |   26,990,659   |   90.47%   | [redpajama-wiki-refine.yaml](redpajama-wiki-refine.yaml)                                                                                                                                                                                                                 | Redpajama               |
| C4                   |         364,868,892         |  344,491,171   |   94.42%   | [redpajama-c4-refine.yaml](redpajama-c4-refine.yaml)                                                                                                                                                                                                                        | Redpajama               |
| Common Crawl 2019-30 |         81,085,420          |   36,557,283   |   45.08%   | [redpajama-cc-2019-30-refine.yaml](redpajama-cc-2019-30-refine.yaml)                                                                                                                                                                             | Redpajama               |
| Common Crawl 2020-05 |         90,850,492          |   42,612,596   |   46.90%   | [redpajama-cc-2020-05-refine.yaml](redpajama-cc-2020-05-refine.yaml)                                                                                                                                                                             | Redpajama               |
| Common Crawl 2021-04 |         98,878,523          |   44,724,752   |   45.23%   | [redpajama-cc-2021-04-refine.yaml](redpajama-cc-2021-04-refine.yaml)                                                                                                                                                                             | Redpajama               |
| Common Crawl 2022-05 |         94,058,868          |   42,648,496   |   45.34%   | [redpajama-cc-2022-05-refine.yaml](redpajama-cc-2022-05-refine.yaml)                                                                                                                                                                             | Redpajama               |
| Common Crawl 2023-06 |         111,402,716         |   50,643,699   |   45.46%   | [redpajama-cc-2023-06-refine.yaml](redpajama-cc-2023-06-refine.yaml)                                                                                                                                                                            | Redpajama               |
| Github Code          | 73,208,524 <br>+ 21,387,703 |   49,279,344   |   52.09%   | [redpajama-code-refine.yaml](github_code/redpajama-code-refine.yaml)<br>[stack-code-refine.yaml](github_code/stack-code-refine.yaml)<br>[redpajama-stack-code-deduplicate.yaml](github_code/redpajama-stack-code-deduplicate.yaml) redpajama-stack-code-refined-by-data-juicer)                             | Redpajama<br>The Stack  |
| StackExchange        |         45,447,328          |   26,309,203   |   57.89%   | [redpajama-pile-stackexchange-refine.yaml](redpajama-pile-stackexchange-refine.yaml)                                                                                                                                                            | Redpajama<br>The Pile   |
| EuroParl             |           69,814            |     61,601     |   88.23%   | [pile-europarl-refine.yaml](pile-europarl-refine.yaml)                                                                                                                                                                                                               | The Pile                |
| FreeLaw              |          3,562,015          |   2,942,612    |   82.61%   | [pile-freelaw-refine.yaml](pile-freelaw-refine.yaml)                                                                                                                                                                                                                    | The Pile                |
| HackerNews           |           373,027           |    371,331     |   99.55%   | [pile-hackernews-refine.yaml](pile-hackernews-refine.yaml)                                                                                                                                                                                                       | The Pile                |
| NIH ExPorter         |           939,661           |    858,492     |   91.36%   | [pile-nih-refine.yaml](pile-nih-refine.yaml)                                                                                                                                                                                                                                   | The Pile                |
| PhilPapers           |           32,782            |     29,117     |   88.82%   | [pile-philpaper-refine.yaml](pile-philpaper-refine.yaml)                                                                                                                                                                                                           | The Pile                |
| PubMed Abstracts     |         15,518,009          |   15,009,325   |   96.72%   | [pile-pubmed-abstract-refine.yaml](pile-pubmed-abstract-refine.yaml)                                                                                                                                                                                   | The Pile                |
| PubMed Central       |          3,098,930          |   2,694,860    |   86.96%   | [pile-pubmed-central-refine.yaml](pile-pubmed-central-refine.yaml)                                                                                                                                                                                         | The Pile                |
| USPTO                |          5,883,024          |   4,516,283    |   76.77%   | [pile-uspto-refine.yaml](pile-uspto-refine.yaml)                                                                                                                                                                                    | The Pile                |


## Before and after refining for Alpaca-CoT Dataset

| subset | #samples before     |             #samples after             | keep ratio | config link                                                                                                                                                                                                                                                                                                                                                                                 | source                 |
|------------------|:-------------------------:|:--------------------------------------:|:----------:|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|
| Alpaca-Cot EN | 136,219,879               | 72,855,345 |   54.48%   | [alpaca-cot-en-refine.yaml](alpaca_cot/alpaca-cot-en-refine.yaml)                                                                                                                                                                                                   | [39 Subsets of Alpaca-CoT](alpaca_cot/README.md#refined-alpaca-cot-dataset-meta-info)              |
| Alpaca-Cot ZH | 21,197,246               |               9,873,214                |   46.58%   | [alpaca-cot-zh-refine.yaml](alpaca_cot/alpaca-cot-zh-refine.yaml)                                                                                                                                                                                                   | [28 Subsets of Alpaca-CoT](alpaca_cot/README.md#refined-alpaca-cot-dataset-meta-info)              |

## Before and after refining for Multimodal Dataset

| subset                    |       #samples before       | #samples after | keep ratio | config link                                                                                                                                                                                                                                                                                                           | source        |
|---------------------------|:---------------------------:|:--------------:|:----------:|--------------------------------------|---------------|
| LLaVA pretrain (LCS-558k) |          558,128          |   500,380    |   89.65%   | [llava-pretrain-refine.yaml](llava-pretrain-refine.yaml)                                         | [LLaVA-1.5](https://github.com/haotian-liu/LLaVA) |
| Data-Juicer-T2V |          1,217,346          |   147,176    |   12.09%   | [2_multi_op_pipline.yaml](../demo/bench/2_multi_op_pipline.yaml)                                        | [InternVid (606k)](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid) <br> [Panda-70M (605k)](https://github.com/snap-research/Panda-70M) <br> [MSR-VTT (6k)](https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/) |

## For Video Dataset

We provide a video dataset processing recipe example for users to better utilize video-related OPs in [general-video-refine-example.yaml](general-video-refine-example.yaml). Here we apply three types of OPs:
- Text-Only: to improve the dataset quality according to the video captions.
- Video-Only: to improve the dataset quality according to the video features.
- Text-Video: to improve the dataset quality according to the alignment between text and videos.
Users can start to process their video datasets based on this recipe.
