# Process config example for dataset
# global parameters
project_name: 'demo-process'
dataset_path: '/Users/zhanglongbin/pythonWork/csghub-dataflow-main/demo/input'  # path to your dataset directory or file
np: 2  # number of subprocess to process your dataset
open_tracer: True
export_path: '/Users/zhanglongbin/pythonWork/csghub-dataflow-main/demo/output/_df_dataset.jsonl'
user_name: depenglee
user_token: ab0033c38611468f8807f2f26013d5d1
repo_id: depenglee/testdepeng
branch: v2
# executor_type: ray
open_tracer: True
# ray_address: 'ray://192.168.31.110:10001'
# ray_address: 'ray://47.93.24.244:30718'
# export_shard_size: 100

# process schedule
# a list of several process operators with their arguments
process:
  # - language_id_score_filter:
  #     lang: 'zh'
  #     min_score: 0.8
  # - document_deduplicator:
  #     lowercase: True
  #     ignore_non_character: True
  - random_selector:
      select_ratio: 0.5
      select_num: 5  
#   - remove_words_with_incorrect_substrings_mapper:          # remove words with incorrect substrings from text.
#       lang: en                                                # sample in which language
#       tokenization: false                                     # whether to use model to tokenize documents
#       substrings: ['http', 'www', '.com', 'href', '//']
#   - remove_specific_chars_mapper:          # remove words with incorrect substrings from text.
#       chars_to_remove: ['◆●■►▼▲▴∆▻▷❖♡□']
#   - clean_email_mapper:
#   - specified_field_filter:
#       field_key: ''                                           # the target key corresponding to multi-level field information need to be separated by '.'
#       target_value: [] 
#   - document_minhash_deduplicator:
#       tokenization: 'character'
  # - extract_qa_mapper:                                      # mapper to extract question and answer pair from text.
  #     hf_model: 'alibaba-pai/pai-qwen1_5-7b-doc2qa'           # model name on huggingface to extract question and answer pair.
  #     pattern: null                                           # regular expression pattern to search for within text.
  #     qa_format: 'chatml'                                     # Output format of question and answer pair.
  #     enable_vllm: false                                       # Whether to use vllm for inference acceleration.
  #     tensor_parallel_size: null                              # It is only valid when enable_vllm is True. The number of GPUs to use for distributed execution with tensor parallelism.
  #     max_model_len: null                                     # It is only valid when enable_vllm is True. Model context length. If unspecified, will be automatically derived from the model config.
  #     max_num_seqs: 256                                       # It is only valid when enable_vllm is True. Maximum number of sequences to be processed in a single iteration.
  #     sampling_params: {}  
#   - optimize_instruction_mapper:                            # optimize instruction.
#       hf_model: 'alibaba-pai/Qwen2-7B-Instruct-Refine'        # model name on huggingface to optimize instruction
#       enable_vllm: false                                       # whether to use vllm for inference acceleration.
#       tensor_parallel_size: null                              # It is only valid when enable_vllm is True. The number of GPUs to use for distributed execution with tensor parallelism.
#       max_model_len: null                                     # It is only valid when enable_vllm is True. Model context length. If unspecified, will be automatically derived from the model config.
#       max_num_seqs: 256                                       # It is only valid when enable_vllm is True. Maximum number of sequences to be processed in a single iteration.
#       sampling_params: {} 